# Full Stack Engineer Demo Solution - Normal Mode (KAG + Conversational Buffer)
name: "Full Stack Development Assistant - Normal Mode"
description: "Intelligent code review, testing, and deployment assistant using KAG with LLM-powered analysis"
solution_type: "normal"  # Uses KAG + Conversational Buffer Memory

# Workflows included in this solution
workflows:
  - name: "Code Review Pipeline"
    file: "fullstack_code_review_workflow.yaml"
    priority: 1
    description: "Analyzes code quality, security, and generates improvements"
    
  - name: "API Development Workflow"
    file: "fullstack_api_development_workflow.yaml"
    priority: 2
    description: "Complete API development with auto-documentation"
    
  - name: "TDD Pipeline"
    file: "fullstack_tdd_workflow.yaml"
    priority: 3
    description: "Test-driven development with coverage reporting"

# Tools available to all workflows
tools:
  - code_analyzer
  - debug_assistant
  - test_generator
  - deployment_checker
  - api_documentation_generator

# Solution configuration
configuration:
  intelligence_mode: "normal"
  memory_type: "conversational_buffer"
  llm_provider: "gemini"
  fact_extraction: true
  reasoning_enabled: true
  
# Use cases for this solution
use_cases:
  - "Code quality assurance and review"
  - "Automated testing and coverage analysis"
  - "API documentation generation"
  - "Deployment readiness validation"
  - "Security vulnerability detection"
  
# Expected outcomes
outcomes:
  quality_improvement: "30-50% reduction in code issues"
  test_coverage: "80-95% coverage achieved"
  deployment_confidence: "High confidence with comprehensive checks"
  documentation_completeness: "100% API endpoint documentation"

# Demo scenario
demo_scenario:
  title: "Real-World Code Review Sprint"
  description: |
    A full-stack engineer submits a new feature implementation with:
    - FastAPI backend endpoints
    - Database integration code
    - Frontend API client
    
    The solution will:
    1. Analyze code quality and find security issues
    2. Identify bugs and suggest fixes
    3. Generate comprehensive unit tests
    4. Create API documentation
    5. Validate deployment readiness
    
    **Why Normal Mode?**
    - LLM reasoning provides intelligent code analysis
    - Extracts meaningful facts about code patterns
    - Generates context-aware recommendations
    - Better understanding of complex code logic
    
  sample_input:
    code: |
      from fastapi import FastAPI, HTTPException
      from pydantic import BaseModel
      
      app = FastAPI()
      users_db = {}
      
      class User(BaseModel):
          id: int
          name: str
          email: str
      
      @app.post("/users")
      def create_user(user: User):
          if user.id in users_db:
              raise HTTPException(400, "User exists")
          users_db[user.id] = user
          return {"status": "created", "user": user}
      
      @app.get("/users/{user_id}")
      def get_user(user_id: int):
          if user_id not in users_db:
              raise HTTPException(404, "User not found")
          return users_db[user_id]
    
    language: "python"
    framework: "fastapi"

# Performance expectations (Normal Mode)
performance:
  llm_calls_per_workflow: "4-6 calls"
  processing_time: "2-3 seconds per workflow"
  cost_per_workflow: "~$0.05"
  quality_score: "95% (High quality with LLM reasoning)"
  
# Client presentation talking points
presentation_points:
  - "**Intelligent Code Analysis**: Uses Gemini LLM to understand code context and patterns"
  - "**Security-First**: Automatically detects eval(), SQL injection, XSS vulnerabilities"
  - "**Test Automation**: Generates pytest/jest tests with 80%+ coverage target"
  - "**API Documentation**: Auto-generates OpenAPI/Swagger docs from code"
  - "**Memory Continuity**: Learns from previous workflows to provide better recommendations"
  - "**Production Ready**: Validates deployment checklist before go-live"
